#!/usr/bin/env python

import os
import time
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
import uvicorn

# Импортируем llama-cpp для работы с gguf моделями
from llama_cpp import Llama

# Импортируем GoogleTranslator из deep_translator для перевода
from deep_translator import GoogleTranslator

app = FastAPI()

# Настройка CORS (разрешены запросы со всех источников)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Директория, где находятся gguf модели
MODELS_DIR = "models"

def scan_models():
    """
    Сканирует папку MODELS_DIR и возвращает список моделей (файлов с расширением .gguf).
    """
    models = []
    try:
        for file in os.listdir(MODELS_DIR):
            if file.endswith(".gguf"):
                model_id = os.path.splitext(file)[0]
                models.append({
                    "id": model_id,
                    "name": f"Model {model_id}",
                    "path": os.path.join(MODELS_DIR, file)
                })
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail=f"Directory '{MODELS_DIR}' not found")
    return models

# Глобальный кэш для загруженных моделей, чтобы не загружать модель каждый раз заново
loaded_models = {}

def get_model_instance(model_id: str, model_path: str) -> Llama:
    """
    Возвращает экземпляр модели Llama для указанного model_id.
    Если модель ещё не загружена, создаёт и сохраняет её в кэше.
    """
    if model_id in loaded_models:
        return loaded_models[model_id]
    try:
        # Настройте параметр n_ctx и другие параметры в зависимости от вашей модели
        instance = Llama(model_path=model_path, n_ctx=2048)
        loaded_models[model_id] = instance
        return instance
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load model '{model_id}': {e}")

# Определения моделей для входящих данных чата
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float
    max_tokens: int
    stream: bool

class ChatResponseChoice(BaseModel):
    message: ChatMessage
    finish_reason: str

class ChatResponseUsage(BaseModel):
    total_tokens: int

class ChatResponse(BaseModel):
    choices: List[ChatResponseChoice]
    usage: ChatResponseUsage

# Определения моделей для запроса перевода
class TranslateRequest(BaseModel):
    text: str
    targetLang: str

class TranslateResponse(BaseModel):
    translatedText: str

@app.get("/v1/models")
def get_models():
    """
    Эндпоинт возвращает список доступных моделей.
    Сканирует папку MODELS_DIR и возвращает файлы с расширением .gguf.
    """
    models = scan_models()
    return {"data": models}

@app.post("/v1/chat/completions")
def chat_completions(req: ChatRequest):
    """
    Эндпоинт для генерации ответа от LLM.
    Используется llama-cpp-python для генерации текста на основе переданного диалога.
    """
    models = scan_models()
    selected_model = next((m for m in models if m["id"] == req.model), None)
    if not selected_model:
        raise HTTPException(status_code=404, detail="Selected model not found")
    
    if not req.messages:
        raise HTTPException(status_code=400, detail="No messages provided")
    
    # Формируем prompt из диалога
    prompt = ""
    for message in req.messages:
        role = "User" if message.role.lower() == "user" else "Assistant"
        prompt += f"{role}: {message.content}\n"
    
    # Получаем экземпляр модели
    llm_instance = get_model_instance(selected_model["id"], selected_model["path"])
    
    try:
        start_time = time.time()
        result = llm_instance(
            prompt=prompt,
            max_tokens=req.max_tokens if req.max_tokens > 0 else 2048,
            temperature=req.temperature,
            stop=["\nUser:"]
        )
        end_time = time.time()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM generation failed: {e}")
    
    # Извлекаем сгенерированный текст
    if result.get("choices") and result["choices"]:
        reply_text = result["choices"][0].get("text", "").strip()
    else:
        reply_text = "No response generated by model."
    
    total_tokens = len(prompt.split()) + len(reply_text.split())
    
    return ChatResponse(
        choices=[ChatResponseChoice(
            message=ChatMessage(role="assistant", content=reply_text),
            finish_reason="stop"
        )],
        usage=ChatResponseUsage(total_tokens=total_tokens)
    )

def split_text(text, max_length=500):
    """
    Разбивает текст на части, чтобы каждая часть была не длиннее max_length.
    Деление происходит по предложениям.
    """
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        # Добавляем точку обратно, если предложение непустое
        sentence = sentence.strip()
        if sentence:
            sentence += "."
        if len(current_chunk) + len(sentence) + 1 <= max_length:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

@app.post("/v1/translate")
def translate(req: TranslateRequest):
    """
    Эндпоинт для перевода текста.
    Если текст слишком длинный, разбивает его на части и переводит каждую отдельно.
    """
    try:
        dest_lang = req.targetLang if req.targetLang else 'ru'
        # Определяем порог, выше которого текст делим на части
        MAX_LENGTH = 500
        if len(req.text) > MAX_LENGTH:
            chunks = split_text(req.text, MAX_LENGTH)
            translated_chunks = []
            for chunk in chunks:
                translated_chunk = GoogleTranslator(source='auto', target=dest_lang).translate(chunk)
                translated_chunks.append(translated_chunk)
            translated_text = " ".join(translated_chunks)
        else:
            translated_text = GoogleTranslator(source='auto', target=dest_lang).translate(req.text)
        return TranslateResponse(translatedText=translated_text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Translation failed: {e}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=3005)
